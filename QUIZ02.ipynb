{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"QUIZ02.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz02-Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.      6.1101 17.592 ]\n",
      " [ 1.      5.5277  9.1302]\n",
      " [ 1.      8.5186 13.662 ]\n",
      " [ 1.      7.0032 11.854 ]\n",
      " [ 1.      5.8598  6.8233]]\n",
      "[nan nan nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    " # start by loading the data\n",
    "dataQ1 = pd.read_csv(\"Data1.txt\", header = None, \n",
    "names = [\"Exam 1 Score\", \"Exam 2 Score\", \"Accepted\"])\n",
    " # initialize some useful variables\n",
    "m = len(dataQ1[\"Accepted\"])\n",
    "x0 = np.ones(m)\n",
    "size = np.array((dataQ1[\"Exam 1 Score\"]))\n",
    "bedrooms = np.array((dataQ1[\"Exam 2 Score\"]))\n",
    "XQ1 = np.array([x0, size, bedrooms]).T\n",
    "yQ1 = np.array(dataQ1[\"Accepted\"])\n",
    "print(XQ1[:5])\n",
    "print(yQ1[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Logistic Regression in Python.\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "---\n",
    "Logistic regression is predicting in which category a given data point is. In binary classifiction, there are only two categories:\n",
    "\n",
    "$$y \\in \\{0,1\\}$$\n",
    "\n",
    "In this question, you will implement logistic regression and apply it to two different datasets.\n",
    "### Sigmoid\n",
    "The sigmoid function, or logistic function, is a function that asymptotes at 0 and 1. The value at 0 is $\\frac{1}{2}$.\n",
    "\n",
    "$h_\\theta(x) = g(\\theta^Tx) = g(z) = \\frac{1}{1+ e^{-z}} = \\frac{1}{1+ e^{-\\theta^Tx}}$\n",
    "\n",
    "We are going to use the sigmoid function to predict how likely it is that a given data point is in category 0. Our hypothesis:\n",
    "\n",
    "$h_\\theta(x) = P(y = 0|x;\\theta)$\n",
    "\n",
    "Because there are only two categories (in this case), we can derrive that:\n",
    "\n",
    "$P(y = 0|x;\\theta) + P(y = 1|x;\\theta)= 1$\n",
    "\n",
    "\n",
    "#### The cost function\n",
    "\n",
    "The cost function in logistic regression differs from the one used in linear regression. The cost function in logistic regression:\n",
    "\n",
    "$$J(\\theta) = - \\begin{bmatrix}\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}-y^{(i)}\\log h(x^{(i)}-(1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))\\end{bmatrix}$$\n",
    "\n",
    "Assume our hypothesis for an example is wrong, the higher probability $h_\\theta$ had predicted, the higher the penatly.\n",
    "\n",
    "A vectorized version of the cost function:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} ⋅(−y^T \\log(h)−(1−y)^T \\log(1−h))$$\n",
    "\n",
    "#### The gradient\n",
    "\n",
    "The gradient is the step a minimization algorithm, like gradient descent, takes to get to the (local) minimum. Note that this step can be taken in a higher dimension and hence the gradient is a vector. This time, we are going to use an algorithm called conjugate gradient to find the minimum. How that algorithm works is beyond the scope if this course. If you are interested, you can learn more about it [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method).\n",
    "\n",
    "The partial derrivative or $J(\\theta)$, i.e., Gradient:\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\theta_J} = {\\nabla_{\\theta} J(\\theta)} = \\frac{1}{m}\\displaystyle\\sum_{i = 1}^{m} \\begin{bmatrix}(h_\\theta(x^{(i)}) - y^{(i)}\\end{bmatrix}x_j^{(i)}$$\n",
    "\n",
    "Vectorized from of Gradient:\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\theta_J} = {\\nabla_{\\theta} J(\\theta)} = \\frac{1}{m} \\cdot X^T \\cdot (g(X\\cdot\\theta)-\\vec{y})$$\n",
    "\n",
    "# Question 1: Implement the function of Logistic Regression that would return cost and gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def logistic_regression_cost_and_gradient(theta, X, y):\n",
    "   \n",
    "    m = y.shape[0]\n",
    "    h = sigmoid(X @ theta)\n",
    "    h = np.clip(h, 1e-8, 1 - 1e-8)\n",
    "    # cost\n",
    "    J = (1 / m) * (-y.T @ np.log(h) - (1 - y).T @ np.log(1 - h))\n",
    "\n",
    "    # gradient\n",
    "    grad = (1 / m) * (X.T @ (h - y))\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "[nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "initial_theta = np.zeros(XQ1.shape[1])\n",
    "cost, gradient = logistic_regression_cost_and_gradient(initial_theta, XQ1, yQ1)\n",
    "print(cost)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed! 🌈</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Newton's method for logistic regression\n",
    "\n",
    "Newton's method addresses getting to $f(\\theta) = 0$, and minimizing $J(\\theta)$ means getting $\\frac{\\partial J}{\\partial \\theta}$ to 0. There after applying Newton's method, extending it to multidimensional setting (Newton-Raphson method), the update rule becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta &:= \\theta - \\frac{\\partial J(\\theta) / \\partial \\theta} {H} \\\\\n",
    "       &:= \\theta - \\frac{\\nabla_{\\theta} J(\\theta)} {H} \\\\\n",
    "       &:= \\theta - H^{-1} \\nabla_{\\theta} J(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "${\\nabla_{\\theta} J(\\theta)}$ is defined in the question 1, and $H$ is the Hessian matrix, given as:\n",
    "\n",
    "$$\n",
    "H_{ij} = \\frac{1}{m} \\sum_{k=1}^{m} g(z^{(k)})\\big(1 - g(z^{(k)})\\big) x_i^{(k)} x_j^{(k)}\n",
    "$$\n",
    "\n",
    "# Question 2: Implement the function to create a Hessian Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_hessian(X, theta):\n",
    "  \n",
    "    m = X.shape[0]\n",
    "    h = sigmoid(X @ theta)\n",
    "    #diagonal\n",
    "    R = np.diag((h * (1 - h)).flatten())\n",
    "    # Hessian\n",
    "    H = (1 / m) * (X.T @ R @ X)\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros((X.shape[1], 1))\n",
    "H = compute_hessian(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.50000000e-01,  1.36947271e-02,  4.57753898e-02,\n",
       "         6.18938338e-02, -6.36796007e-03,  7.53424034e-02,\n",
       "         1.49583323e-02,  7.67038337e-03,  3.87062793e-03,\n",
       "         3.55875032e-02,  3.06346073e-02, -1.31275953e-03,\n",
       "         1.26082186e-02, -2.76204706e-03,  4.27746262e-02,\n",
       "         1.29912680e-02,  2.95294996e-03,  2.35802361e-03,\n",
       "         4.56945212e-03,  1.02227103e-03,  2.89274082e-02,\n",
       "         1.95927957e-02, -1.75695667e-04,  4.73335083e-03,\n",
       "        -4.26239309e-04,  5.64792459e-03, -1.57545194e-03,\n",
       "         3.14314006e-02],\n",
       "       [ 1.36947271e-02,  6.18938338e-02, -6.36796007e-03,\n",
       "         1.49583323e-02,  7.67038337e-03,  3.87062793e-03,\n",
       "         3.06346073e-02, -1.31275953e-03,  1.26082186e-02,\n",
       "        -2.76204706e-03,  1.29912680e-02,  2.95294996e-03,\n",
       "         2.35802361e-03,  4.56945212e-03,  1.02227103e-03,\n",
       "         1.95927957e-02, -1.75695667e-04,  4.73335083e-03,\n",
       "        -4.26239309e-04,  5.64792459e-03, -1.57545194e-03,\n",
       "         1.14603386e-02,  1.58359405e-03,  1.49586976e-03,\n",
       "         1.38037635e-03,  7.51568407e-04,  3.12301466e-03,\n",
       "        -3.85029858e-05],\n",
       "       [ 4.57753898e-02, -6.36796007e-03,  7.53424034e-02,\n",
       "         7.67038337e-03,  3.87062793e-03,  3.55875032e-02,\n",
       "        -1.31275953e-03,  1.26082186e-02, -2.76204706e-03,\n",
       "         4.27746262e-02,  2.95294996e-03,  2.35802361e-03,\n",
       "         4.56945212e-03,  1.02227103e-03,  2.89274082e-02,\n",
       "        -1.75695667e-04,  4.73335083e-03, -4.26239309e-04,\n",
       "         5.64792459e-03, -1.57545194e-03,  3.14314006e-02,\n",
       "         1.58359405e-03,  1.49586976e-03,  1.38037635e-03,\n",
       "         7.51568407e-04,  3.12301466e-03, -3.85029858e-05,\n",
       "         2.57335544e-02],\n",
       "       [ 6.18938338e-02,  1.49583323e-02,  7.67038337e-03,\n",
       "         3.06346073e-02, -1.31275953e-03,  1.26082186e-02,\n",
       "         1.29912680e-02,  2.95294996e-03,  2.35802361e-03,\n",
       "         4.56945212e-03,  1.95927957e-02, -1.75695667e-04,\n",
       "         4.73335083e-03, -4.26239309e-04,  5.64792459e-03,\n",
       "         1.14603386e-02,  1.58359405e-03,  1.49586976e-03,\n",
       "         1.38037635e-03,  7.51568407e-04,  3.12301466e-03,\n",
       "         1.46097110e-02,  2.20307539e-04,  2.32662432e-03,\n",
       "        -4.64342908e-05,  1.78921268e-03, -1.78149681e-04,\n",
       "         3.40839294e-03],\n",
       "       [-6.36796007e-03,  7.67038337e-03,  3.87062793e-03,\n",
       "        -1.31275953e-03,  1.26082186e-02, -2.76204706e-03,\n",
       "         2.95294996e-03,  2.35802361e-03,  4.56945212e-03,\n",
       "         1.02227103e-03, -1.75695667e-04,  4.73335083e-03,\n",
       "        -4.26239309e-04,  5.64792459e-03, -1.57545194e-03,\n",
       "         1.58359405e-03,  1.49586976e-03,  1.38037635e-03,\n",
       "         7.51568407e-04,  3.12301466e-03, -3.85029858e-05,\n",
       "         2.20307539e-04,  2.32662432e-03, -4.64342908e-05,\n",
       "         1.78921268e-03, -1.78149681e-04,  3.40839294e-03,\n",
       "        -1.11625563e-03],\n",
       "       [ 7.53424034e-02,  3.87062793e-03,  3.55875032e-02,\n",
       "         1.26082186e-02, -2.76204706e-03,  4.27746262e-02,\n",
       "         2.35802361e-03,  4.56945212e-03,  1.02227103e-03,\n",
       "         2.89274082e-02,  4.73335083e-03, -4.26239309e-04,\n",
       "         5.64792459e-03, -1.57545194e-03,  3.14314006e-02,\n",
       "         1.49586976e-03,  1.38037635e-03,  7.51568407e-04,\n",
       "         3.12301466e-03, -3.85029858e-05,  2.57335544e-02,\n",
       "         2.32662432e-03, -4.64342908e-05,  1.78921268e-03,\n",
       "        -1.78149681e-04,  3.40839294e-03, -1.11625563e-03,\n",
       "         2.69705364e-02],\n",
       "       [ 1.49583323e-02,  3.06346073e-02, -1.31275953e-03,\n",
       "         1.29912680e-02,  2.95294996e-03,  2.35802361e-03,\n",
       "         1.95927957e-02, -1.75695667e-04,  4.73335083e-03,\n",
       "        -4.26239309e-04,  1.14603386e-02,  1.58359405e-03,\n",
       "         1.49586976e-03,  1.38037635e-03,  7.51568407e-04,\n",
       "         1.46097110e-02,  2.20307539e-04,  2.32662432e-03,\n",
       "        -4.64342908e-05,  1.78921268e-03, -1.78149681e-04,\n",
       "         1.04643451e-02,  1.08650778e-03,  9.99778500e-04,\n",
       "         5.49461855e-04,  4.70446744e-04,  8.34867180e-04,\n",
       "         2.32578828e-04],\n",
       "       [ 7.67038337e-03, -1.31275953e-03,  1.26082186e-02,\n",
       "         2.95294996e-03,  2.35802361e-03,  4.56945212e-03,\n",
       "        -1.75695667e-04,  4.73335083e-03, -4.26239309e-04,\n",
       "         5.64792459e-03,  1.58359405e-03,  1.49586976e-03,\n",
       "         1.38037635e-03,  7.51568407e-04,  3.12301466e-03,\n",
       "         2.20307539e-04,  2.32662432e-03, -4.64342908e-05,\n",
       "         1.78921268e-03, -1.78149681e-04,  3.40839294e-03,\n",
       "         1.08650778e-03,  9.99778500e-04,  5.49461855e-04,\n",
       "         4.70446744e-04,  8.34867180e-04,  2.32578828e-04,\n",
       "         2.40452695e-03],\n",
       "       [ 3.87062793e-03,  1.26082186e-02, -2.76204706e-03,\n",
       "         2.35802361e-03,  4.56945212e-03,  1.02227103e-03,\n",
       "         4.73335083e-03, -4.26239309e-04,  5.64792459e-03,\n",
       "        -1.57545194e-03,  1.49586976e-03,  1.38037635e-03,\n",
       "         7.51568407e-04,  3.12301466e-03, -3.85029858e-05,\n",
       "         2.32662432e-03, -4.64342908e-05,  1.78921268e-03,\n",
       "        -1.78149681e-04,  3.40839294e-03, -1.11625563e-03,\n",
       "         9.99778500e-04,  5.49461855e-04,  4.70446744e-04,\n",
       "         8.34867180e-04,  2.32578828e-04,  2.40452695e-03,\n",
       "        -4.03838764e-04],\n",
       "       [ 3.55875032e-02, -2.76204706e-03,  4.27746262e-02,\n",
       "         4.56945212e-03,  1.02227103e-03,  2.89274082e-02,\n",
       "        -4.26239309e-04,  5.64792459e-03, -1.57545194e-03,\n",
       "         3.14314006e-02,  1.38037635e-03,  7.51568407e-04,\n",
       "         3.12301466e-03, -3.85029858e-05,  2.57335544e-02,\n",
       "        -4.64342908e-05,  1.78921268e-03, -1.78149681e-04,\n",
       "         3.40839294e-03, -1.11625563e-03,  2.69705364e-02,\n",
       "         5.49461855e-04,  4.70446744e-04,  8.34867180e-04,\n",
       "         2.32578828e-04,  2.40452695e-03, -4.03838764e-04,\n",
       "         2.45830420e-02],\n",
       "       [ 3.06346073e-02,  1.29912680e-02,  2.95294996e-03,\n",
       "         1.95927957e-02, -1.75695667e-04,  4.73335083e-03,\n",
       "         1.14603386e-02,  1.58359405e-03,  1.49586976e-03,\n",
       "         1.38037635e-03,  1.46097110e-02,  2.20307539e-04,\n",
       "         2.32662432e-03, -4.64342908e-05,  1.78921268e-03,\n",
       "         1.04643451e-02,  1.08650778e-03,  9.99778500e-04,\n",
       "         5.49461855e-04,  4.70446744e-04,  8.34867180e-04,\n",
       "         1.21131470e-02,  4.16569144e-04,  1.32968662e-03,\n",
       "         2.58694384e-05,  7.60075030e-04,  1.72084087e-05,\n",
       "         9.32212619e-04],\n",
       "       [-1.31275953e-03,  2.95294996e-03,  2.35802361e-03,\n",
       "        -1.75695667e-04,  4.73335083e-03, -4.26239309e-04,\n",
       "         1.58359405e-03,  1.49586976e-03,  1.38037635e-03,\n",
       "         7.51568407e-04,  2.20307539e-04,  2.32662432e-03,\n",
       "        -4.64342908e-05,  1.78921268e-03, -1.78149681e-04,\n",
       "         1.08650778e-03,  9.99778500e-04,  5.49461855e-04,\n",
       "         4.70446744e-04,  8.34867180e-04,  2.32578828e-04,\n",
       "         4.16569144e-04,  1.32968662e-03,  2.58694384e-05,\n",
       "         7.60075030e-04,  1.72084087e-05,  9.32212619e-04,\n",
       "        -1.25140376e-04],\n",
       "       [ 1.26082186e-02,  2.35802361e-03,  4.56945212e-03,\n",
       "         4.73335083e-03, -4.26239309e-04,  5.64792459e-03,\n",
       "         1.49586976e-03,  1.38037635e-03,  7.51568407e-04,\n",
       "         3.12301466e-03,  2.32662432e-03, -4.64342908e-05,\n",
       "         1.78921268e-03, -1.78149681e-04,  3.40839294e-03,\n",
       "         9.99778500e-04,  5.49461855e-04,  4.70446744e-04,\n",
       "         8.34867180e-04,  2.32578828e-04,  2.40452695e-03,\n",
       "         1.32968662e-03,  2.58694384e-05,  7.60075030e-04,\n",
       "         1.72084087e-05,  9.32212619e-04, -1.25140376e-04,\n",
       "         2.47997405e-03],\n",
       "       [-2.76204706e-03,  4.56945212e-03,  1.02227103e-03,\n",
       "        -4.26239309e-04,  5.64792459e-03, -1.57545194e-03,\n",
       "         1.38037635e-03,  7.51568407e-04,  3.12301466e-03,\n",
       "        -3.85029858e-05, -4.64342908e-05,  1.78921268e-03,\n",
       "        -1.78149681e-04,  3.40839294e-03, -1.11625563e-03,\n",
       "         5.49461855e-04,  4.70446744e-04,  8.34867180e-04,\n",
       "         2.32578828e-04,  2.40452695e-03, -4.03838764e-04,\n",
       "         2.58694384e-05,  7.60075030e-04,  1.72084087e-05,\n",
       "         9.32212619e-04, -1.25140376e-04,  2.47997405e-03,\n",
       "        -8.66050064e-04],\n",
       "       [ 4.27746262e-02,  1.02227103e-03,  2.89274082e-02,\n",
       "         5.64792459e-03, -1.57545194e-03,  3.14314006e-02,\n",
       "         7.51568407e-04,  3.12301466e-03, -3.85029858e-05,\n",
       "         2.57335544e-02,  1.78921268e-03, -1.78149681e-04,\n",
       "         3.40839294e-03, -1.11625563e-03,  2.69705364e-02,\n",
       "         4.70446744e-04,  8.34867180e-04,  2.32578828e-04,\n",
       "         2.40452695e-03, -4.03838764e-04,  2.45830420e-02,\n",
       "         7.60075030e-04,  1.72084087e-05,  9.32212619e-04,\n",
       "        -1.25140376e-04,  2.47997405e-03, -8.66050064e-04,\n",
       "         2.55216695e-02],\n",
       "       [ 1.29912680e-02,  1.95927957e-02, -1.75695667e-04,\n",
       "         1.14603386e-02,  1.58359405e-03,  1.49586976e-03,\n",
       "         1.46097110e-02,  2.20307539e-04,  2.32662432e-03,\n",
       "        -4.64342908e-05,  1.04643451e-02,  1.08650778e-03,\n",
       "         9.99778500e-04,  5.49461855e-04,  4.70446744e-04,\n",
       "         1.21131470e-02,  4.16569144e-04,  1.32968662e-03,\n",
       "         2.58694384e-05,  7.60075030e-04,  1.72084087e-05,\n",
       "         9.89357094e-03,  8.98940220e-04,  7.00089702e-04,\n",
       "         2.59085644e-04,  2.92514323e-04,  2.99477839e-04,\n",
       "         1.77141278e-04],\n",
       "       [ 2.95294996e-03, -1.75695667e-04,  4.73335083e-03,\n",
       "         1.58359405e-03,  1.49586976e-03,  1.38037635e-03,\n",
       "         2.20307539e-04,  2.32662432e-03, -4.64342908e-05,\n",
       "         1.78921268e-03,  1.08650778e-03,  9.99778500e-04,\n",
       "         5.49461855e-04,  4.70446744e-04,  8.34867180e-04,\n",
       "         4.16569144e-04,  1.32968662e-03,  2.58694384e-05,\n",
       "         7.60075030e-04,  1.72084087e-05,  9.32212619e-04,\n",
       "         8.98940220e-04,  7.00089702e-04,  2.59085644e-04,\n",
       "         2.92514323e-04,  2.99477839e-04,  1.77141278e-04,\n",
       "         5.78688795e-04],\n",
       "       [ 2.35802361e-03,  4.73335083e-03, -4.26239309e-04,\n",
       "         1.49586976e-03,  1.38037635e-03,  7.51568407e-04,\n",
       "         2.32662432e-03, -4.64342908e-05,  1.78921268e-03,\n",
       "        -1.78149681e-04,  9.99778500e-04,  5.49461855e-04,\n",
       "         4.70446744e-04,  8.34867180e-04,  2.32578828e-04,\n",
       "         1.32968662e-03,  2.58694384e-05,  7.60075030e-04,\n",
       "         1.72084087e-05,  9.32212619e-04, -1.25140376e-04,\n",
       "         7.00089702e-04,  2.59085644e-04,  2.92514323e-04,\n",
       "         2.99477839e-04,  1.77141278e-04,  5.78688795e-04,\n",
       "         3.55836364e-05],\n",
       "       [ 4.56945212e-03, -4.26239309e-04,  5.64792459e-03,\n",
       "         1.38037635e-03,  7.51568407e-04,  3.12301466e-03,\n",
       "        -4.64342908e-05,  1.78921268e-03, -1.78149681e-04,\n",
       "         3.40839294e-03,  5.49461855e-04,  4.70446744e-04,\n",
       "         8.34867180e-04,  2.32578828e-04,  2.40452695e-03,\n",
       "         2.58694384e-05,  7.60075030e-04,  1.72084087e-05,\n",
       "         9.32212619e-04, -1.25140376e-04,  2.47997405e-03,\n",
       "         2.59085644e-04,  2.92514323e-04,  2.99477839e-04,\n",
       "         1.77141278e-04,  5.78688795e-04,  3.55836364e-05,\n",
       "         2.02581709e-03],\n",
       "       [ 1.02227103e-03,  5.64792459e-03, -1.57545194e-03,\n",
       "         7.51568407e-04,  3.12301466e-03, -3.85029858e-05,\n",
       "         1.78921268e-03, -1.78149681e-04,  3.40839294e-03,\n",
       "        -1.11625563e-03,  4.70446744e-04,  8.34867180e-04,\n",
       "         2.32578828e-04,  2.40452695e-03, -4.03838764e-04,\n",
       "         7.60075030e-04,  1.72084087e-05,  9.32212619e-04,\n",
       "        -1.25140376e-04,  2.47997405e-03, -8.66050064e-04,\n",
       "         2.92514323e-04,  2.99477839e-04,  1.77141278e-04,\n",
       "         5.78688795e-04,  3.55836364e-05,  2.02581709e-03,\n",
       "        -4.81685439e-04],\n",
       "       [ 2.89274082e-02, -1.57545194e-03,  3.14314006e-02,\n",
       "         3.12301466e-03, -3.85029858e-05,  2.57335544e-02,\n",
       "        -1.78149681e-04,  3.40839294e-03, -1.11625563e-03,\n",
       "         2.69705364e-02,  8.34867180e-04,  2.32578828e-04,\n",
       "         2.40452695e-03, -4.03838764e-04,  2.45830420e-02,\n",
       "         1.72084087e-05,  9.32212619e-04, -1.25140376e-04,\n",
       "         2.47997405e-03, -8.66050064e-04,  2.55216695e-02,\n",
       "         2.99477839e-04,  1.77141278e-04,  5.78688795e-04,\n",
       "         3.55836364e-05,  2.02581709e-03, -4.81685439e-04,\n",
       "         2.47506319e-02],\n",
       "       [ 1.95927957e-02,  1.14603386e-02,  1.58359405e-03,\n",
       "         1.46097110e-02,  2.20307539e-04,  2.32662432e-03,\n",
       "         1.04643451e-02,  1.08650778e-03,  9.99778500e-04,\n",
       "         5.49461855e-04,  1.21131470e-02,  4.16569144e-04,\n",
       "         1.32968662e-03,  2.58694384e-05,  7.60075030e-04,\n",
       "         9.89357094e-03,  8.98940220e-04,  7.00089702e-04,\n",
       "         2.59085644e-04,  2.92514323e-04,  2.99477839e-04,\n",
       "         1.08515158e-02,  5.46421217e-04,  8.41810282e-04,\n",
       "         3.63357950e-05,  3.81263951e-04,  3.72329906e-05,\n",
       "         3.48250425e-04],\n",
       "       [-1.75695667e-04,  1.58359405e-03,  1.49586976e-03,\n",
       "         2.20307539e-04,  2.32662432e-03, -4.64342908e-05,\n",
       "         1.08650778e-03,  9.99778500e-04,  5.49461855e-04,\n",
       "         4.70446744e-04,  4.16569144e-04,  1.32968662e-03,\n",
       "         2.58694384e-05,  7.60075030e-04,  1.72084087e-05,\n",
       "         8.98940220e-04,  7.00089702e-04,  2.59085644e-04,\n",
       "         2.92514323e-04,  2.99477839e-04,  1.77141278e-04,\n",
       "         5.46421217e-04,  8.41810282e-04,  3.63357950e-05,\n",
       "         3.81263951e-04,  3.72329906e-05,  3.48250425e-04,\n",
       "         1.49187049e-05],\n",
       "       [ 4.73335083e-03,  1.49586976e-03,  1.38037635e-03,\n",
       "         2.32662432e-03, -4.64342908e-05,  1.78921268e-03,\n",
       "         9.99778500e-04,  5.49461855e-04,  4.70446744e-04,\n",
       "         8.34867180e-04,  1.32968662e-03,  2.58694384e-05,\n",
       "         7.60075030e-04,  1.72084087e-05,  9.32212619e-04,\n",
       "         7.00089702e-04,  2.59085644e-04,  2.92514323e-04,\n",
       "         2.99477839e-04,  1.77141278e-04,  5.78688795e-04,\n",
       "         8.41810282e-04,  3.63357950e-05,  3.81263951e-04,\n",
       "         3.72329906e-05,  3.48250425e-04,  1.49187049e-05,\n",
       "         5.95988952e-04],\n",
       "       [-4.26239309e-04,  1.38037635e-03,  7.51568407e-04,\n",
       "        -4.64342908e-05,  1.78921268e-03, -1.78149681e-04,\n",
       "         5.49461855e-04,  4.70446744e-04,  8.34867180e-04,\n",
       "         2.32578828e-04,  2.58694384e-05,  7.60075030e-04,\n",
       "         1.72084087e-05,  9.32212619e-04, -1.25140376e-04,\n",
       "         2.59085644e-04,  2.92514323e-04,  2.99477839e-04,\n",
       "         1.77141278e-04,  5.78688795e-04,  3.55836364e-05,\n",
       "         3.63357950e-05,  3.81263951e-04,  3.72329906e-05,\n",
       "         3.48250425e-04,  1.49187049e-05,  5.95988952e-04,\n",
       "        -1.15276312e-04],\n",
       "       [ 5.64792459e-03,  7.51568407e-04,  3.12301466e-03,\n",
       "         1.78921268e-03, -1.78149681e-04,  3.40839294e-03,\n",
       "         4.70446744e-04,  8.34867180e-04,  2.32578828e-04,\n",
       "         2.40452695e-03,  7.60075030e-04,  1.72084087e-05,\n",
       "         9.32212619e-04, -1.25140376e-04,  2.47997405e-03,\n",
       "         2.92514323e-04,  2.99477839e-04,  1.77141278e-04,\n",
       "         5.78688795e-04,  3.55836364e-05,  2.02581709e-03,\n",
       "         3.81263951e-04,  3.72329906e-05,  3.48250425e-04,\n",
       "         1.49187049e-05,  5.95988952e-04, -1.15276312e-04,\n",
       "         2.04555984e-03],\n",
       "       [-1.57545194e-03,  3.12301466e-03, -3.85029858e-05,\n",
       "        -1.78149681e-04,  3.40839294e-03, -1.11625563e-03,\n",
       "         8.34867180e-04,  2.32578828e-04,  2.40452695e-03,\n",
       "        -4.03838764e-04,  1.72084087e-05,  9.32212619e-04,\n",
       "        -1.25140376e-04,  2.47997405e-03, -8.66050064e-04,\n",
       "         2.99477839e-04,  1.77141278e-04,  5.78688795e-04,\n",
       "         3.55836364e-05,  2.02581709e-03, -4.81685439e-04,\n",
       "         3.72329906e-05,  3.48250425e-04,  1.49187049e-05,\n",
       "         5.95988952e-04, -1.15276312e-04,  2.04555984e-03,\n",
       "        -6.68330681e-04],\n",
       "       [ 3.14314006e-02, -3.85029858e-05,  2.57335544e-02,\n",
       "         3.40839294e-03, -1.11625563e-03,  2.69705364e-02,\n",
       "         2.32578828e-04,  2.40452695e-03, -4.03838764e-04,\n",
       "         2.45830420e-02,  9.32212619e-04, -1.25140376e-04,\n",
       "         2.47997405e-03, -8.66050064e-04,  2.55216695e-02,\n",
       "         1.77141278e-04,  5.78688795e-04,  3.55836364e-05,\n",
       "         2.02581709e-03, -4.81685439e-04,  2.47506319e-02,\n",
       "         3.48250425e-04,  1.49187049e-05,  5.95988952e-04,\n",
       "        -1.15276312e-04,  2.04555984e-03, -6.68330681e-04,\n",
       "         2.57349590e-02]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed! 🎉</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression\n",
    "\n",
    "---\n",
    "Regularization is a meganism for preventing overfitting. _Overfitting_ means that our model works extremely well on the training set but bad in the real world. It's focussed on the training data. _Underfitting_ is either a not well trained model or the feature mapping is not done (correctly). We will use regularization in this exercise. Furtermore, we are going to look at a more complex decision boundary\n",
    "\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assur- ance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.051267,  0.69956 ],\n",
       "       [-0.092742,  0.68494 ],\n",
       "       [-0.21371 ,  0.69225 ],\n",
       "       [-0.375   ,  0.50219 ],\n",
       "       [-0.51325 ,  0.46564 ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start by loading the data\n",
    "data = pd.read_csv(\"Data2.txt\", header = None, \n",
    "                   names = [\"Test 1\", \"Test 2\", \"Status\"])\n",
    "\n",
    "# initialize some useful variables\n",
    "m = len(data[\"Status\"])\n",
    "size = np.array((data[\"Test 1\"]))\n",
    "bedrooms = np.array((data[\"Test 2\"]))\n",
    "X = np.array([size, bedrooms]).T # don't add a column of ones yet.\n",
    "y = np.array(data[\"Status\"])\n",
    "\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping\n",
    "\n",
    "One way to fit the data better is to create more features from each data point. While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. This will also add $x_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature(X1, X2, degree):\n",
    "    if not type(X1) == np.ndarray:\n",
    "        X1 = np.array([X1])\n",
    "\n",
    "    if not type(X2) == np.ndarray:\n",
    "        X2 = np.array([X2])\n",
    "\n",
    "    assert X1.shape == X2.shape\n",
    "    \n",
    "    out = np.ones((len(X1), 1))\n",
    "    for i in range(1, degree+1):\n",
    "        for j in range(i + 1):\n",
    "            new = (X1 ** (i-j) * X2 ** j).reshape(len(X1), 1)\n",
    "            out = np.hstack((out, new))\n",
    "    return out\n",
    "\n",
    "X = map_feature(X[:,0], X[:,1], degree=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Computing Cost\n",
    "#### Cost function\n",
    "Regularization works by penalizing theta. Theta values can be high when an overfit occurs so we prefer to keep them low. $\\lambda$ is the regularization parameter. If $\\lambda=0$, no regularization happens. If $\\lambda$ is some big number, $\\theta$ has a very high penalty. Just like the learning rate $\\alpha$ you have to try out certain values and see which work.\n",
    "\n",
    "The cost function with regularization:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}-\\begin{bmatrix}y^{(i)}\\log h(x^{(i)}+(1-y^{(i)}\\log(1-h_\\theta(x^{(i)})) \\end{bmatrix} + \\frac{\\lambda}{m}\\displaystyle\\sum_{j=1}^{n}{\\theta_j}^2$$\n",
    "\n",
    "# Question 3: Implement the function of Regularized Logistic Regression cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def regularized_logistic_regression_cost(theta, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute the regularized cost and gradient for logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "    - theta: Parameters of the model (n x 1 or n, )\n",
    "    - X: Feature matrix (m x n)\n",
    "    - y: Labels (m x 1 or m, )\n",
    "    - lambda_: Regularization parameter (scalar)\n",
    "\n",
    "    Returns:\n",
    "    - J: Regularized cost (scalar)\n",
    "    - reg_term: Regularized term (scalar)\n",
    "    \"\"\"\n",
    "    # number of training samples\n",
    "    m = len(y)\n",
    "\n",
    "    z = np.dot(X, theta)\n",
    "    h = (1 / (1 + np.exp(-z)))\n",
    "    h = np.clip(h, 1e-8, 1 - 1e-8)\n",
    "    cost =  (-1/m) * ((y @ np.log(h) + (1 - y) @ np.log(1 - h)))\n",
    "    reg_term = (lambda_ / (2 * m)) * np.sum(np.square(theta[1:]))\n",
    "    \n",
    "    return cost, reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed! ✨</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Regularized Gradient descent\n",
    "Gradient descent, just like the cost function, is slightly modified for regularization.\n",
    "\n",
    "For $\\theta_{j}$ where $j = 0$: \n",
    "\n",
    "$$\\theta_j := \\theta_j -\\alpha \\begin{bmatrix}\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}(h_\\theta(x^{(i)}-y^{(i)}){x_0}^{(i)}\\end{bmatrix}$$\n",
    "\n",
    "For $\\theta_{j}$ where $j \\in \\{1, 2, ..., n\\} $: \n",
    "\n",
    "$$\\theta_j := \\theta_j -\\alpha \\begin{bmatrix}\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}(h_\\theta(x^{(i)}-y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}{\\theta_j} \\end{bmatrix}$$\n",
    "\n",
    "Note that we don't penalize our bias vector $X_1$.\n",
    "\n",
    "# Question 4: Implement the function of Regularized Logistic Regression gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def regularized_gradient_descent(X, y, theta, alpha, lambda_, num_iters):\n",
    "    m = len(y)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        z = np.dot(X, theta)\n",
    "        h = (1 / (1 + np.exp(-z)))\n",
    "\n",
    "        error = h - y  # shape: (m,)\n",
    "        grad = (1 / m) * np.dot(X.T, error)  # shape: (n,)\n",
    "\n",
    "        # Regularize all theta except theta[0]\n",
    "        reg_term = (lambda_ / m) * theta\n",
    "        reg_term[0] = 0  # Do not regularize the bias term\n",
    "\n",
    "        grad += reg_term\n",
    "\n",
    "        # Update theta\n",
    "        theta -= alpha * grad\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed! 🍀</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = np.array([[1, 2, 3], [1, 3, 4], [1, 5, 6]])\n>>> y = np.array([0, 1, 0])\n>>> initial_theta = np.zeros(X.shape[1])\n>>> cost, gradient = logistic_regression_cost_and_gradient(initial_theta, X, y)\n>>> assert np.isclose(cost, 0.6931471805599453, atol=1e-06), 'The value is not correct'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X = np.array([[1, 2, 3], [1, 3, 4], [1, 5, 6]])\n>>> y = np.array([0, 1, 0])\n>>> initial_theta = np.zeros(X.shape[1])\n>>> cost, gradient = logistic_regression_cost_and_gradient(initial_theta, X, y)\n>>> assert np.isclose(gradient[0], 0.16666667, atol=1e-06), 'The value is not correct'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X = np.array([[1, 2, 3], [1, 3, 4], [1, 5, 6]])\n>>> y = np.array([0, 1, 0])\n>>> initial_theta = np.zeros(X.shape[1])\n>>> cost, gradient = logistic_regression_cost_and_gradient(initial_theta, X, y)\n>>> assert np.isclose(gradient[1], 0.66666667, atol=1e-06), 'The value is not correct'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X = np.array([[1, 2, 3], [1, 3, 4], [1, 5, 6]])\n>>> y = np.array([0, 1, 0])\n>>> initial_theta = np.zeros(X.shape[1])\n>>> cost, gradient = logistic_regression_cost_and_gradient(initial_theta, X, y)\n>>> assert np.isclose(gradient[2], 0.83333333, atol=1e-06), 'The value is not correct'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X = np.array([[1, 2, 3], [1, 3, 4], [1, 5, 6]])\n>>> initial_theta = np.zeros(X.shape[1])\n>>> H = compute_hessian(X, initial_theta)\n>>> assert np.allclose(H, H.T, atol=1e-08), 'Hessian matrix is not symmetrical'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X = np.array([[1, 2, 3], [1, 3, 4], [1, 5, 6]])\n>>> initial_theta = np.zeros(X.shape[1])\n>>> H = compute_hessian(X, initial_theta)\n>>> assert np.isclose(H[0][0], 0.25, atol=1e-08), 'The value is not 0.25'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> lambda_ = 1.0\n>>> initial_theta = np.ones(X.shape[1])\n>>> J, reg_term = regularized_logistic_regression_cost(initial_theta, X, y, lambda_)\n>>> assert np.isclose(J, 2.02044153500484), 'The value is not correct'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> lambda_ = 1.0\n>>> initial_theta = np.ones(X.shape[1])\n>>> J, reg_term = regularized_logistic_regression_cost(initial_theta, X, y, lambda_)\n>>> assert np.isclose(reg_term, 0.11440677966101695), 'The value is not correct'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> theta = np.ones(X.shape[1])\n>>> alpha = 0.1\n>>> lambda_ = 1.0\n>>> num_iters = 10\n>>> theta_learned = regularized_gradient_descent(X, y, theta, alpha, lambda_, num_iters)\n>>> assert np.isclose(theta_learned[0], 0.6755423), 'The value is not correct'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
