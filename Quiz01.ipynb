{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'otter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Otter\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01motter\u001b[39;00m\n\u001b[0;32m      3\u001b[0m grader \u001b[38;5;241m=\u001b[39m otter\u001b[38;5;241m.\u001b[39mNotebook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUIZ1.ipynb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'otter'"
     ]
    }
   ],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"QUIZ1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz01-Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population   Profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('Data1.txt', header=None, names=['Population', 'Profit'])\n",
    "\n",
    "# Prepare feature matrix and target variable\n",
    "m = len(data)\n",
    "\n",
    "# Initialize X, y and theta\n",
    "x0 = np.ones(m)\n",
    "population = np.array((data[\"Population\"]))\n",
    "X = np.array([x0, population]).T\n",
    "y = np.array(data[\"Profit\"]).reshape(len(data.index), 1)\n",
    "\n",
    "# Display first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Question 1: Implement the Cost Function for Linear Regression\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "You are given a dataset (`Data1.txt`) with two columns:\n",
    "- `Population` (Feature \\( X \\))\n",
    "- `Profit` (Target \\( y \\))\n",
    "\n",
    "### **Task**\n",
    "Implement the function `cost_function(X, y, theta)`, which computes the cost function for linear regression.\n",
    "\n",
    "### **Formula**\n",
    "$$J(\\theta) = \\frac{1}{2m} (X\\theta - y)^T (X\\theta - y)\\$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32.07273388]]\n"
     ]
    }
   ],
   "source": [
    "def cost_function(X, y, theta):\n",
    "    #m = len(y)\n",
    "    change = ( X @ theta ) - y\n",
    "    cost = 1/(2*m) * (change.T) @ ( X @ theta - y)\n",
    "    return cost\n",
    "print(cost_function(X, y, np.array([[0], [0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgrader\u001b[49m\u001b[38;5;241m.\u001b[39mcheck(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grader' is not defined"
     ]
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Question 2: Implement Gradient Descent\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "### **Formula for Gradient Update**\n",
    "$$ \\theta := \\theta - \\frac{\\alpha}{m} X^T (X\\theta - y)\\ $$\n",
    "\n",
    "Now that we have implemented the cost function in Question 1, we will implement **gradient descent** to minimize $J(\\theta)\\$.\n",
    "\n",
    "### **Task**\n",
    "Implement the function `gradient_descent(X, y, theta, alpha, num_iters)`, which performs **batch gradient descent**.\n",
    "\n",
    "### **Formula for Gradient Update**\n",
    "$$ \\theta := \\theta - \\frac{\\alpha}{m} X^T (X\\theta - y)\\ $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0086469]\n",
      " [0.8007915]]\n",
      "[6.73719046 5.93159357 5.90115471 5.89522859 5.89009494 5.88500416\n",
      " 5.87993248 5.87487909 5.86984391 5.86482687]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\InaequoSolutions-PC\\AppData\\Local\\Temp\\ipykernel_2324\\2111401385.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cost_history[i] = cost\n"
     ]
    }
   ],
   "source": [
    "def Batch_gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn theta.\n",
    "\n",
    "    Parameters:\n",
    "    X : numpy array of shape (m, n) - Feature matrix\n",
    "    y : numpy array of shape (m, 1) - Target vector\n",
    "    theta : numpy array of shape (n, 1) - Parameter vector\n",
    "    alpha : float - Learning rate\n",
    "    num_iters : int - Number of iterations\n",
    "\n",
    "    Returns:\n",
    "    theta : numpy array of shape (n, 1) - Optimized parameters\n",
    "    cost_history : list - Cost function values at each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(num_iters)\n",
    "    for i in range(num_iters):\n",
    "        gradient = (1 / m) * (X.T) @ ( X @ theta - y)\n",
    "        theta -= alpha * gradient\n",
    "        cost = cost_function(X, y, theta)\n",
    "        cost_history[i] = cost\n",
    "    \n",
    "    return theta, cost_history\n",
    " \n",
    "theta, cost_history = Batch_gradient_descent(X, y, np.zeros((2, 1), dtype=np.float64), 0.01, 10)\n",
    "print(theta)\n",
    "print(cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgrader\u001b[49m\u001b[38;5;241m.\u001b[39mcheck(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grader' is not defined"
     ]
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3: Features Normalization for Multivariate Linear Regression\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "When features differ by order of magnitude, performing feature scaling can make **gradient descent** converge much more quickly. Formally:\n",
    "\n",
    "$$ x := \\frac{x - \\mu}{\\sigma}\\ $$\n",
    "\n",
    "Where:  \n",
    "- $ \\mu $ is the **mean** of the feature.  \n",
    "- $\\sigma$ is the **standard deviation** of the feature.  \n",
    "\n",
    "ðŸš¨ **Important**: Store $ \\mu $ and $ \\sigma $, as they are needed later for making predictions.\n",
    "\n",
    "### **Task**\n",
    "Perform **feature normalization** on the **House Data** features from the dataset `Data2.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>3</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>2</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Size  Bedrooms   Price\n",
       "0  2104         3  399900\n",
       "1  1600         3  329900\n",
       "2  2400         3  369000\n",
       "3  1416         2  232000\n",
       "4  3000         4  539900"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"Data2.txt\", header = None, names=[\"Size\", \"Bedrooms\",\"Price\"])\n",
    "m = len(data)\n",
    "\n",
    "# Initialize X, y and theta\n",
    "x0 = np.ones(m)\n",
    "size = np.array((data[\"Size\"]))\n",
    "bedrooms = np.array((data[\"Bedrooms\"]))\n",
    "X_mult = np.array([x0, size, bedrooms]).T\n",
    "y_mult = np.array(data[\"Price\"]).reshape(len(data.index), 1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668.2836879432624\n",
      "1045.7916905133081\n"
     ]
    }
   ],
   "source": [
    "def normalize(X):\n",
    "    \"\"\" Normalizes the features in X\n",
    "    \n",
    "    returns a normalized version of X where\n",
    "    the mean value of each feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when\n",
    "    working with learning algorithms.\n",
    "    \"\"\"\n",
    "    mu = np.zeros(len(X))\n",
    "    sigma = np.zeros(len(X))\n",
    "    \n",
    "    mu = X.mean()\n",
    "    sigma = X.std()\n",
    "    X_norm = (X - mu) / sigma\n",
    "    \n",
    "    return X_norm, mu, sigma  # Return all values\n",
    "X_norm, mu, sigma = normalize(X_mult)\n",
    "print(mu)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgrader\u001b[49m\u001b[38;5;241m.\u001b[39mcheck(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grader' is not defined"
     ]
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 4: Stochastic Gradient Descent\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "In this question, you will implement Stochastic Gradient Descent Algorithm with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "### **Theory**\n",
    "Gradient descent is an iterative optimization algorithm used to minimize the cost function:\n",
    "\n",
    "$$\n",
    "repeat \\ \\{ \\ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\displaystyle\\sum_{i = 1}^{m}(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\\ \\}\n",
    "$$\n",
    "\n",
    "For multiple features, this can be vectorized as:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\frac{\\alpha}{m} X^T (X\\theta - \\vec{y})\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ X $ is the feature matrix,  \n",
    "- $ y $ is the target vector,  \n",
    "- $ \\theta $ is the parameter vector,  \n",
    "- $ \\alpha $ is the learning rate,  \n",
    "- $ m $ is the number of training examples.\n",
    "\n",
    "### **Task**\n",
    "- Implement the **Stochastic gradient descent** for multiple features.\n",
    "- The function should support any number of features.\n",
    "- Return the optimized $ \\theta $ values and the cost history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-102780.69696113]\n",
      " [ 159998.84543563]\n",
      " [-102487.24970359]]\n",
      "-5803.440296243021\n"
     ]
    }
   ],
   "source": [
    "def Stochastic_gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn theta.\n",
    "\n",
    "    Parameters:\n",
    "    X : numpy array of shape (m, n) - Feature matrix\n",
    "    y : numpy array of shape (m, 1) - Target vector\n",
    "    theta : numpy array of shape (n, 1) - Parameter vector\n",
    "    alpha : float - Learning rate\n",
    "    num_iters : int - Number of iterations\n",
    "\n",
    "    Returns:\n",
    "    theta : numpy array of shape (n, 1) - Optimized parameters\n",
    "    cost_history : list - Cost function values at each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(num_iters * m)\n",
    "    np.random.seed(1234)  # Fix random seed for reproducibility\n",
    "    for i in range(num_iters):\n",
    "        for j in range(m):\n",
    "            rand_index = np.random.randint(0, m)\n",
    "            X_j = X[rand_index:rand_index+1]\n",
    "            y_j = y[rand_index:rand_index+1]\n",
    "            gradient = X_j.T @ ((X_j @ theta) - y_j)\n",
    "            theta -= alpha * gradient\n",
    "            cost_history[i * m + j] = (1/ m) *  np.sum((X @ (theta) - y))\n",
    "    return theta, cost_history\n",
    "X_norm, mu, sigma = normalize(X_mult) \n",
    "theta, cost_history = Stochastic_gradient_descent(X_norm, y_mult, np.zeros((3, 1), dtype=np.float64), 0.01, 10)\n",
    "print(theta)\n",
    "print(cost_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(cost_function(X, y, np.array([[0], [0]])), 32.07273388)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(cost_function(X, y, np.array([[1.0], [1.0]])), 10.26652049)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> theta, cost_history = Batch_gradient_descent(X, y, np.zeros((2, 1), dtype=np.float64), 0.01, 10)\n>>> assert np.isclose(theta[0], 0.0086469)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> theta, cost_history = Batch_gradient_descent(X, y, np.zeros((2, 1), dtype=np.float64), 0.01, 10)\n>>> assert len(cost_history) == 10\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_norm, mu, sigma = normalize(X_mult)\n>>> assert np.isclose(mu, 668.2836879432624)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_norm, mu, sigma = normalize(X_mult)\n>>> theta, cost_history = Stochastic_gradient_descent(X_norm, y_mult, np.zeros((3, 1), dtype=np.float64), 0.01, 10)\n>>> assert np.isclose(theta[0], -102780.69696113)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_norm, mu, sigma = normalize(X_mult)\n>>> theta, cost_history = Stochastic_gradient_descent(X_norm, y_mult, np.zeros((3, 1), dtype=np.float64), 0.01, 10)\n>>> assert len(cost_history) == 470\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
