{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b2af58-2aa9-4cd8-87af-aee3784e35db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install required packages\n",
    "!pip install transformers datasets torchvision tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fe912a-2a18-4a55-9180-f560f251d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    BlipProcessor, BlipForConditionalGeneration,\n",
    "    AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000a46ce-ef59-4959-afa6-d98e10c7421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    \"\"\"Fixed Dataset class with proper tokenization\"\"\"\n",
    "    \n",
    "    def __init__(self, data, processor, max_length=128):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            if isinstance(item['image'], str) and item['image'].startswith('http'):\n",
    "                response = requests.get(item['image'], stream=True, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                image = Image.open(response.raw)\n",
    "            else:\n",
    "                image = item['image'] if hasattr(item['image'], 'convert') else Image.open(item['image'])\n",
    "            \n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "            # Process image\n",
    "            pixel_values = self.processor(image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "            \n",
    "            # Process text - FIXED: Proper tokenization\n",
    "            text = item['text']\n",
    "            \n",
    "            # Tokenize text with proper parameters\n",
    "            text_inputs = self.processor.tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"input_ids\": text_inputs[\"input_ids\"].squeeze(0),  # Remove batch dimension\n",
    "                \"attention_mask\": text_inputs[\"attention_mask\"].squeeze(0)  # Remove batch dimension\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {idx}: {e}\")\n",
    "            # Return a dummy sample to avoid crashing\n",
    "            dummy_image = Image.new('RGB', (224, 224), color='white')\n",
    "            pixel_values = self.processor(dummy_image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "            \n",
    "            dummy_text = \"a photo\"\n",
    "            text_inputs = self.processor.tokenizer(\n",
    "                dummy_text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"input_ids\": text_inputs[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": text_inputs[\"attention_mask\"].squeeze(0)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba10550-dd43-4060-970f-48c8823c822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionGenerator:\n",
    "    \"\"\"Fixed Image Caption Generator with proper training\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Salesforce/blip-image-captioning-base\"):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load processor and model\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(f\"Model loaded: {model_name}\")\n",
    "    \n",
    "    def load_dataset(self, dataset_name=\"nlphuji/flickr30k\", split=\"test\", sample_size=100):\n",
    "        \"\"\"Load and prepare dataset - reduced sample size for stability\"\"\"\n",
    "        print(f\"Loading dataset: {dataset_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load dataset\n",
    "            dataset = load_dataset(dataset_name, split=split)\n",
    "            \n",
    "            # Sample smaller subset for stability\n",
    "            if len(dataset) > sample_size:\n",
    "                indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "                dataset = dataset.select(indices)\n",
    "            \n",
    "            # Convert to list for easier manipulation\n",
    "            data = []\n",
    "            for item in tqdm(dataset, desc=\"Processing dataset\"):\n",
    "                try:\n",
    "                    # Handle different dataset formats\n",
    "                    if 'caption' in item:\n",
    "                        caption = item['caption']\n",
    "                    elif 'text' in item:\n",
    "                        caption = item['text']\n",
    "                    elif 'captions' in item:\n",
    "                        # Take first caption if multiple exist\n",
    "                        caption = item['captions'][0] if isinstance(item['captions'], list) else item['captions']\n",
    "                    else:\n",
    "                        caption = \"A photo\"\n",
    "                    \n",
    "                    # Clean caption text - IMPORTANT FIX\n",
    "                    if isinstance(caption, list):\n",
    "                        caption = caption[0]\n",
    "                    caption = str(caption).strip()\n",
    "                    \n",
    "                    if len(caption) > 0:  # Only add if caption is not empty\n",
    "                        data.append({\n",
    "                            'image': item['image'],\n",
    "                            'text': caption\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping problematic item: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"Dataset loaded with {len(data)} valid samples\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "            print(\"Creating sample data for demonstration...\")\n",
    "            \n",
    "            # Create sample data if dataset loading fails\n",
    "            sample_urls = [\n",
    "                \"https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=400\",\n",
    "                \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\", \n",
    "                \"https://images.unsplash.com/photo-1571019613454-1cb2f99b2d8b?w=400\",\n",
    "                \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\",\n",
    "                \"https://images.unsplash.com/photo-1574158622682-e40e69881006?w=400\"\n",
    "            ]\n",
    "            \n",
    "            sample_captions = [\n",
    "                \"A dog sitting on grass\",\n",
    "                \"Mountain landscape with snow\",\n",
    "                \"City skyline at night\",\n",
    "                \"A cute puppy playing\",\n",
    "                \"Beautiful nature scene\"\n",
    "            ]\n",
    "            \n",
    "            data = []\n",
    "            for url, caption in zip(sample_urls, sample_captions):\n",
    "                data.append({'image': url, 'text': caption})\n",
    "            \n",
    "            print(f\"Created sample dataset with {len(data)} samples\")\n",
    "            return data\n",
    "    \n",
    "    def split_data(self, data, train_ratio=0.8, random_state=42):\n",
    "        \"\"\"Split data into train and test sets\"\"\"\n",
    "        train_data, test_data = train_test_split(\n",
    "            data, \n",
    "            train_size=train_ratio, \n",
    "            random_state=random_state,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Train samples: {len(train_data)}\")\n",
    "        print(f\"Test samples: {len(test_data)}\")\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    def create_dataloaders(self, train_data, test_data, batch_size=2):\n",
    "        \"\"\"Create data loaders with smaller batch size for stability\"\"\"\n",
    "        train_dataset = ImageCaptionDataset(train_data, self.processor)\n",
    "        test_dataset = ImageCaptionDataset(test_data, self.processor)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
    "            pin_memory=False,  # Disable for stability\n",
    "            drop_last=True  # Drop incomplete batches\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    def train(self, train_loader, test_loader, epochs=2, lr=5e-5):\n",
    "        \"\"\"Fixed training function with proper error handling\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        # Put model in training mode\n",
    "        self.model.train()\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(0.1 * total_steps),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            successful_batches = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                try:\n",
    "                    # Move to device\n",
    "                    pixel_values = batch['pixel_values'].to(self.device)\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    \n",
    "                    # Debug: Print shapes\n",
    "                    if batch_idx == 0:\n",
    "                        print(f\"Pixel values shape: {pixel_values.shape}\")\n",
    "                        print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "                        print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "                    \n",
    "                    # Forward pass - FIXED: Use proper parameters for BLIP\n",
    "                    outputs = self.model(\n",
    "                        pixel_values=pixel_values,\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=input_ids  # Use input_ids as labels for generation\n",
    "                    )\n",
    "                    \n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    if torch.isnan(loss):\n",
    "                        print(f\"NaN loss detected at batch {batch_idx}, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    successful_batches += 1\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping for stability\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': loss.item(),\n",
    "                        'avg_loss': total_loss / max(successful_batches, 1)\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if successful_batches > 0:\n",
    "                avg_loss = total_loss / successful_batches\n",
    "                train_losses.append(avg_loss)\n",
    "                print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f} ({successful_batches} successful batches)\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1} - No successful batches!\")\n",
    "                \n",
    "            # Evaluate on test set\n",
    "            if test_loader and successful_batches > 0:\n",
    "                try:\n",
    "                    test_loss = self.evaluate(test_loader)\n",
    "                    print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Evaluation error: {e}\")\n",
    "        \n",
    "        return train_losses\n",
    "    \n",
    "    def evaluate(self, test_loader):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        successful_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "                try:\n",
    "                    pixel_values = batch['pixel_values'].to(self.device)\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(\n",
    "                        pixel_values=pixel_values,\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=input_ids\n",
    "                    )\n",
    "                    \n",
    "                    if not torch.isnan(outputs.loss):\n",
    "                        total_loss += outputs.loss.item()\n",
    "                        successful_batches += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Evaluation batch error: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return total_loss / max(successful_batches, 1)\n",
    "    \n",
    "    def generate_caption(self, image_path_or_url, max_length=50):\n",
    "        \"\"\"Generate caption for a single image\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            if isinstance(image_path_or_url, str) and image_path_or_url.startswith('http'):\n",
    "                response = requests.get(image_path_or_url, stream=True, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                image = Image.open(response.raw)\n",
    "            else:\n",
    "                image = Image.open(image_path_or_url)\n",
    "            \n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "            # Process image\n",
    "            inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            # Generate caption\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                    temperature=1.0\n",
    "                )\n",
    "            \n",
    "            # Decode caption\n",
    "            caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            return caption\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating caption: {str(e)}\"\n",
    "    \n",
    "    def save_model(self, path=\"./fine_tuned_caption_model\"):\n",
    "        \"\"\"Save the fine-tuned model\"\"\"\n",
    "        try:\n",
    "            self.model.save_pretrained(path)\n",
    "            self.processor.save_pretrained(path)\n",
    "            print(f\"Model saved to {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    def load_model(self, path=\"./fine_tuned_caption_model\"):\n",
    "        \"\"\"Load a fine-tuned model\"\"\"\n",
    "        try:\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(path)\n",
    "            self.processor = BlipProcessor.from_pretrained(path)\n",
    "            self.model.to(self.device)\n",
    "            print(f\"Model loaded from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model from {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155e8d57-823d-40e6-9153-e12a705772d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_new_images():\n",
    "    \"\"\"Example function showing how to test on new images\"\"\"\n",
    "    \n",
    "    # Load the trained model\n",
    "    caption_gen = ImageCaptionGenerator()\n",
    "    \n",
    "    # Option 1: Load your fine-tuned model (if available)\n",
    "    try:\n",
    "        caption_gen.load_model(\"./fine_tuned_caption_model\")\n",
    "        print(\"Loaded fine-tuned model\")\n",
    "    except:\n",
    "        print(\"Using pre-trained model (fine-tuned model not found)\")\n",
    "    \n",
    "    # Test on different types of images\n",
    "    \n",
    "    # Example 1: Test on a local image file\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing on local image:\")\n",
    "    try:\n",
    "        local_image_path = \"image_transformations.png\"  # Replace with your image path\n",
    "        caption = caption_gen.generate_caption(local_image_path)\n",
    "        print(f\"Caption: {caption}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with local image: {e}\")\n",
    "        print(\"Make sure to provide a valid image path\")\n",
    "    \n",
    "    # Example 2: Test on an image URL\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing on image from URL:\")\n",
    "    try:\n",
    "        image_url = \"https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=400\"\n",
    "        caption = caption_gen.generate_caption(image_url)\n",
    "        print(f\"Image URL: {image_url}\")\n",
    "        print(f\"Caption: {caption}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with URL image: {e}\")\n",
    "    \n",
    "    # Example 3: Test multiple images\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing on multiple images:\")\n",
    "    \n",
    "    test_images = [\n",
    "        \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\",  # Dog\n",
    "        \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\",  # Mountain\n",
    "        \"https://images.unsplash.com/photo-1571019613454-1cb2f99b2d8b?w=400\",  # City\n",
    "    ]\n",
    "    \n",
    "    for i, img_url in enumerate(test_images, 1):\n",
    "        try:\n",
    "            caption = caption_gen.generate_caption(img_url, max_length=30)\n",
    "            print(f\"Image {i}: {caption}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with image {i}: {e}\")\n",
    "\n",
    "def interactive_caption_generator():\n",
    "    \"\"\"Interactive function to test images with user input\"\"\"\n",
    "    \n",
    "    caption_gen = ImageCaptionGenerator()\n",
    "    \n",
    "    # Try to load fine-tuned model\n",
    "    try:\n",
    "        caption_gen.load_model(\"./fine_tuned_caption_model\")\n",
    "        print(\"‚úì Loaded fine-tuned model\")\n",
    "    except:\n",
    "        print(\"‚Ñπ Using pre-trained model\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üñºÔ∏è  INTERACTIVE IMAGE CAPTION GENERATOR\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Enter image paths or URLs (type 'quit' to exit)\")\n",
    "    print(\"Examples:\")\n",
    "    print(\"  - Local file: /path/to/image.jpg\")\n",
    "    print(\"  - URL: https://example.com/image.jpg\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter image path or URL: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye! üëã\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            print(\"Please enter a valid path or URL\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(\"üîÑ Generating caption...\")\n",
    "            caption = caption_gen.generate_caption(user_input)\n",
    "            print(f\"üìù Caption: {caption}\")\n",
    "            \n",
    "            # Ask for caption length preference\n",
    "            custom_length = input(\"Want different length? Enter max words (or press Enter): \").strip()\n",
    "            if custom_length.isdigit():\n",
    "                caption = caption_gen.generate_caption(user_input, max_length=int(custom_length))\n",
    "                print(f\"üìù New Caption: {caption}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(\"Make sure the image path/URL is valid and accessible\")\n",
    "\n",
    "def batch_caption_generator(image_folder_path):\n",
    "    \"\"\"Generate captions for all images in a folder\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    caption_gen = ImageCaptionGenerator()\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        caption_gen.load_model(\"./fine_tuned_caption_model\")\n",
    "        print(\"‚úì Loaded fine-tuned model\")\n",
    "    except:\n",
    "        print(\"‚Ñπ Using pre-trained model\")\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'}\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(Path(image_folder_path).glob(f'*{ext}'))\n",
    "        image_files.extend(Path(image_folder_path).glob(f'*{ext.upper()}'))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {image_folder_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "    \n",
    "    # Generate captions\n",
    "    results = []\n",
    "    for img_path in tqdm(image_files, desc=\"Generating captions\"):\n",
    "        try:\n",
    "            caption = caption_gen.generate_caption(str(img_path))\n",
    "            results.append({\n",
    "                'image': img_path.name,\n",
    "                'caption': caption\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {img_path.name}: {e}\")\n",
    "    \n",
    "    # Save results\n",
    "    output_file = Path(image_folder_path) / \"captions.txt\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for result in results:\n",
    "            f.write(f\"{result['image']}: {result['caption']}\\n\")\n",
    "    \n",
    "    print(f\"Captions saved to {output_file}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nGenerated Captions:\")\n",
    "    print(\"-\" * 50)\n",
    "    for result in results:\n",
    "        print(f\"{result['image']}: {result['caption']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0018492-c2cc-4261-ae5d-3ef25b3dc733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Image Caption Generator Training ===\n",
      "Using device: cpu\n",
      "Model loaded: Salesforce/blip-image-captioning-base\n",
      "Loading dataset: nlphuji/flickr30k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 192.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 200 valid samples\n",
      "Train samples: 160\n",
      "Test samples: 40\n",
      "Training batches: 160\n",
      "Test batches: 40\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|                                                                               | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values shape: torch.Size([1, 3, 384, 384])\n",
      "Input IDs shape: torch.Size([1, 128])\n",
      "Attention mask shape: torch.Size([1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 160/160 [18:15<00:00,  6.84s/it, loss=0.833, avg_loss=4.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 4.2512 (160 successful batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [01:13<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Test Loss: 0.6390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:   0%|                                                                               | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values shape: torch.Size([1, 3, 384, 384])\n",
      "Input IDs shape: torch.Size([1, 128])\n",
      "Attention mask shape: torch.Size([1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 160/160 [-5:28:00<00:00, -0.01it/s, loss=0.158, avg_loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Loss: 0.3734 (160 successful batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [01:16<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Test Loss: 0.4687\n",
      "Model saved to ./fine_tuned_caption_model\n",
      "\n",
      "=== Testing Caption Generation ===\n",
      "Generated caption: a brown dog sticks out from under a tree while looking at the camera\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Fixed main training function\"\"\"\n",
    "    print(\"=== Image Caption Generator Training ===\")\n",
    "    \n",
    "    # Initialize\n",
    "    caption_gen = ImageCaptionGenerator()\n",
    "    \n",
    "    # Load smaller dataset for stability\n",
    "    data = caption_gen.load_dataset(sample_size=200)  # Very small for testing\n",
    "    \n",
    "    if len(data) < 4:\n",
    "        print(\"Not enough data for training!\")\n",
    "        return\n",
    "    \n",
    "    # Split data\n",
    "    train_data, test_data = caption_gen.split_data(data, train_ratio=0.8)\n",
    "    \n",
    "    # Create data loaders with small batch size\n",
    "    train_loader, test_loader = caption_gen.create_dataloaders(\n",
    "        train_data, test_data, batch_size=1  # Start with batch size 1\n",
    "    )\n",
    "    \n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        train_losses = caption_gen.train(\n",
    "            train_loader, test_loader, \n",
    "            epochs=2,  # Reduced epochs for testing\n",
    "            lr=5e-5\n",
    "        )\n",
    "        \n",
    "        # Save the model\n",
    "        caption_gen.save_model(\"./fine_tuned_caption_model\")\n",
    "        \n",
    "        # Test generation\n",
    "        print(\"\\n=== Testing Caption Generation ===\")\n",
    "        test_url = \"https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=400\"\n",
    "        caption = caption_gen.generate_caption(test_url)\n",
    "        print(f\"Generated caption: {caption}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cda7132-9894-41bd-a249-130b1b8a39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded: Salesforce/blip-image-captioning-base\n",
      "Model loaded from ./fine_tuned_caption_model\n",
      "Loaded fine-tuned model\n",
      "\n",
      "==================================================\n",
      "Testing on local image:\n",
      "Caption: four different colored squares are shown on a black background with white dots and red, green, blue, yellow\n",
      "\n",
      "==================================================\n",
      "Testing on image from URL:\n",
      "Image URL: https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=400\n",
      "Caption: a brown dog sticks out from under a tree while looking at the camera\n",
      "\n",
      "==================================================\n",
      "Testing on multiple images:\n",
      "Image 1: a brown and white dog is holding a yellow stick in its mouth while sitting on a stone path\n",
      "Image 2: a mountain range with snow and mountains in the background\n",
      "Image 3: a woman in white tank top and orange shorts is sitting on the floor and using her cellphone\n"
     ]
    }
   ],
   "source": [
    "test_on_new_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe120978-ede4-4082-8c8e-90fa9b6e8954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded: Salesforce/blip-image-captioning-base\n",
      "Model loaded from ./fine_tuned_caption_model\n",
      "‚úì Loaded fine-tuned model\n",
      "\n",
      "============================================================\n",
      "üñºÔ∏è  INTERACTIVE IMAGE CAPTION GENERATOR\n",
      "============================================================\n",
      "Enter image paths or URLs (type 'quit' to exit)\n",
      "Examples:\n",
      "  - Local file: /path/to/image.jpg\n",
      "  - URL: https://example.com/image.jpg\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter image path or URL:  https://onlinepngtools.com/images/examples-onlinepngtools/red-petaled-flower.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating caption...\n",
      "üìù Caption: a pink flowered tree with many pink flowers on it\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Want different length? Enter max words (or press Enter):  https://themeisle.com/blog/wp-content/uploads/2024/06/Online-Image-Optimizer-Test-Image-PNG-Version.png\n",
      "\n",
      "Enter image path or URL:  https://tinypng.com/static/images/boat-compressed.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating caption...\n",
      "üìù Caption: a boat is in the water with the sun setting behind it\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Want different length? Enter max words (or press Enter):  quit\n",
      "\n",
      "Enter image path or URL:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye! üëã\n"
     ]
    }
   ],
   "source": [
    "interactive_caption_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b2f4d2-98b3-497e-b352-c609798b87d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded: Salesforce/blip-image-captioning-base\n",
      "Model loaded from ./fine_tuned_caption_model\n",
      "‚úì Loaded fine-tuned model\n",
      "Found 2 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating captions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:24<00:00, 12.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to C:\\Users\\InaequoSolutions-PC\\Downloads\\images_G-AI\\captions.txt\n",
      "\n",
      "Generated Captions:\n",
      "--------------------------------------------------\n",
      "boat-compressed.jpg: a boat is in the water with the sun setting behind it\n",
      "boat-compressed.jpg: a boat is in the water with the sun setting behind it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_caption_generator(r\"C:\\Users\\InaequoSolutions-PC\\Downloads\\images_G-AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b0b2e-f483-4faf-af3c-c5c6b0cadf58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
